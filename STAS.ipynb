{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hv0ln3wkJLlQ"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1652375340101,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "b61bP14R4XM0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.utils.train import Epoch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import random\n",
    "import albumentations as albu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN7bOXMDJPKi"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1652375340101,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "mBgITvPS4XM2"
   },
   "outputs": [],
   "source": [
    "root_dir = './'\n",
    "DATA_DIR_train = os.path.join(root_dir, 'Train')\n",
    "DATA_DIR_vail = os.path.join(root_dir, 'Valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1652375340101,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "nkh_BSqC4XM3"
   },
   "outputs": [],
   "source": [
    "x_train_dir = os.path.join(DATA_DIR_train, 'img')\n",
    "y_train_dir = os.path.join(DATA_DIR_train,'label')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR_vail, 'img')\n",
    "y_valid_dir = os.path.join(DATA_DIR_vail, 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1652375340101,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "YkEXrGoF4XM3"
   },
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1652375342619,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "57Q-vlh8Lbh-"
   },
   "outputs": [],
   "source": [
    "def fix_randomseed(seed):\n",
    "    # Python built-in random module\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    # Cuda\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "fix_randomseed(8863)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1652375342619,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "gJwRNuNO4XM4"
   },
   "outputs": [],
   "source": [
    "class STASDataset(Dataset):\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['bg', 'stas']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        # self.masks_fps[i] = self.masks_fps[i].replace('.jpg','.png')\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if image.shape != (800, 800, 3):\n",
    "            image = cv2.resize(image, (1600, 800), interpolation=cv2.INTER_LANCZOS4)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        if mask.shape != (800, 800, 3):\n",
    "            mask = cv2.resize(mask, (1600, 800), interpolation=cv2.INTER_LANCZOS4)  \n",
    "        mask = mask.astype('bool')\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft5hL8gqJRLS"
   },
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 63642,
     "status": "ok",
     "timestamp": 1652375406259,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "iRDwkOtW4XM5",
    "outputId": "83be4483-a7e9-475c-dea2-4ffa88517f9f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets look at data we have\n",
    "\n",
    "dataset = STASDataset(x_train_dir, y_train_dir, classes=['stas'])\n",
    "\n",
    "image, mask = dataset[4] # get some sample\n",
    "visualize(\n",
    "    image=image, \n",
    "    stas_mask=mask.squeeze(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1652375406676,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "94-j_Vw14XM6"
   },
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.Rotate(limit=40,p=0.5,border_mode=cv2.BORDER_CONSTANT),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=0.5, border_mode=0),\n",
    "        \n",
    "        albu.HueSaturationValue(p=0.6),\n",
    "        albu.Sharpen(p=0.5),\n",
    "        albu.RandomBrightnessContrast(p=0.4),\n",
    "\n",
    "        albu.OneOf([\n",
    "            albu.ElasticTransform(p=0.5, alpha=120, sigma=120*0.05, alpha_affine=120*0.03),\n",
    "            albu.GridDistortion(p=0.5),\n",
    "            albu.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1)\n",
    "        ]),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3205,
     "status": "ok",
     "timestamp": 1652375409880,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "cKwrQ50r4XM7",
    "outputId": "58270633-f2d2-4b84-b68e-3a11dc112c75"
   },
   "outputs": [],
   "source": [
    "#### Visualize resulted augmented images and masks\n",
    "\n",
    "augmented_dataset = STASDataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    classes=['stas'],\n",
    ")\n",
    "\n",
    "# same image with different random transforms\n",
    "for i in range(3):\n",
    "    image, mask = augmented_dataset[1]\n",
    "    visualize(image=image, mask=mask.squeeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cWoHPPx4XM7"
   },
   "source": [
    "## Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "ec5f81c5a86448c1a7ff5913e9e26ef3",
      "e76cab6c71514109b993416a53c2a624",
      "45a55adfb6de48a7a32ce973b0c3189f",
      "a83345383dab49d1997ed5cd62cf3ea7",
      "9da2906e168f48d8bb8053ae8f177c8c",
      "8cb854c2539c4aaab343b3750e20533b",
      "6ae0b4bf834e462bbe9fb0fa3cbd3165",
      "f0e303920ced45299fdc5d377102b98a",
      "51ad174d322c4b9e81b19f7c6a7aac5b",
      "f24b3fc929594204b7016a62ae601213",
      "0e831a91d8194e89bb740cbceb163cae"
     ]
    },
    "executionInfo": {
     "elapsed": 3831,
     "status": "ok",
     "timestamp": 1652375414131,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "FKCo1qYj4XM9",
    "outputId": "b317dae3-5999-44e8-c5f2-b00e70081038"
   },
   "outputs": [],
   "source": [
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['stas']\n",
    "ACTIVATION = 'sigmoid' \n",
    "DEVICE = 'cuda'\n",
    "\n",
    "\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3767,
     "status": "ok",
     "timestamp": 1652375417896,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "3scYPLwt4XM9",
    "outputId": "a0964d25-1f7f-4a9f-e01c-20c5d94200e1"
   },
   "outputs": [],
   "source": [
    "train_dataset = STASDataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "valid_dataset = STASDataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, pin_memory=False, batch_size=16, shuffle=True, num_workers=8)\n",
    "valid_loader = DataLoader(valid_dataset, pin_memory=False, batch_size=16, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference: https://github.com/davda54/sam\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1652375417897,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "c_h5rH454XM9"
   },
   "outputs": [],
   "source": [
    "# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n",
    "loss =smp.utils.losses.BCELoss()\n",
    "metrics = [\n",
    "    smp.utils.metrics.Fscore(threshold=0.5),\n",
    "]\n",
    "init_lr = 3e-4\n",
    "base_optimizer = torch.optim.AdamW  # define an optimizer for the \"sharpness-aware\" update\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM_TrainEpoch(Epoch):\n",
    "    def __init__(self, model, loss, metrics, optimizer, device=\"cpu\", verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name=\"train\",\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def batch_update(self, x, y):\n",
    "        # self.optimizer.zero_grad()\n",
    "        loss = self.loss(self.model.forward(x), y)\n",
    "        loss.backward()\n",
    "        self.optimizer.first_step(zero_grad=True)\n",
    "        prediction = self.model.forward(x)\n",
    "        self.loss(prediction, y).backward()\n",
    "        self.optimizer.second_step(zero_grad=True)\n",
    "        return loss, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9812,
     "status": "ok",
     "timestamp": 1652375427701,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "FleFQ2EO4XM9"
   },
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "\n",
    "train_epoch = SAM_TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1652375427702,
     "user": {
      "displayName": "黃柏翰",
      "userId": "14180742364295487465"
     },
     "user_tz": -480
    },
    "id": "ZHmE4pRl4XM-"
   },
   "outputs": [],
   "source": [
    "path='./Models/'\n",
    "os.makedirs(path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eh5KJBep4XM-",
    "outputId": "51a64b17-2677-4677-9090-e14304c5795a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "\n",
    "train_loss_list = []\n",
    "train_fscore_list = []\n",
    "valid_loss_list = []\n",
    "valid_fcore_list = []\n",
    "EPOCHS = 200\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    train_loss_list.append(train_logs['bce_loss'])\n",
    "    train_fscore_list.append(train_logs['fscore'])\n",
    "    \n",
    "    valid_loss_list.append(valid_logs['bce_loss'])\n",
    "    valid_fcore_list.append(valid_logs['fscore']) \n",
    "  \n",
    "\n",
    "    torch.save(model, path+'Deeplabv3P_resnet50_batch16_new_aug_SAM_epoch200_ori.pth')\n",
    "    print('Model saved!')\n",
    "    if i == 100 or i == 150:\n",
    "        new_lr = init_lr * (1 - (i / EPOCHS))**0.9\n",
    "        optimizer.param_groups[0]['lr'] = new_lr\n",
    "        print(f'Decrease decoder learning rate to {new_lr:.5f}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aI2ca3Yk4XM-"
   },
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.plot(train_fscore_list)\n",
    "plt.plot(valid_fcore_list)\n",
    "plt.title('Train Logs')      \n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(train_loss_list)\n",
    "plt.plot(valid_loss_list)\n",
    "plt.title('Train Logs')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obBUA-ET4XM-"
   },
   "outputs": [],
   "source": [
    "best_model= torch.load('./Models/Deeplabv3P_resnet50_batch16_new_aug_SAM_epoch200_ori.pth')\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vW_phQi7fIMy"
   },
   "source": [
    "## Single Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEZwPEdC4XM_"
   },
   "outputs": [],
   "source": [
    "out_path='./Predictions_Deeplabv3P_resnet50_batch16/'\n",
    "os.makedirs(out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywvLhamrQ6gB"
   },
   "outputs": [],
   "source": [
    "inference_dir = os.path.join(root_dir, 'Inference_Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuHYU9h9QD7d"
   },
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['bg', 'stas']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir,\n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if image.shape != (800, 800, 3):\n",
    "            image = cv2.resize(image, (1600, 800), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image)\n",
    "            image = sample['image']\n",
    "            \n",
    "        return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3tYEdNq4XNA"
   },
   "outputs": [],
   "source": [
    "inference_dataset = InferenceDataset(\n",
    "    inference_dir, \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tophe3ooR1bj"
   },
   "outputs": [],
   "source": [
    "for i in range(len(inference_dataset)):\n",
    "    name = os.path.basename(inference_dataset.images_fps[i])\n",
    "    image_vis = inference_dataset[i][0]\n",
    "    image = inference_dataset[i]\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        pr_mask = best_model(x_tensor)\n",
    "        pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "    zeros = np.zeros((800, 1600, 3))\n",
    "    zeros[...,0] = pr_mask\n",
    "    zeros[...,1] = pr_mask\n",
    "    zeros[...,2] = pr_mask\n",
    "    zeros = cv2.resize(zeros, (1716, 942))\n",
    "    plt.imsave(os.path.join(out_path, name.replace('.jpg','.png')), zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model Inference\n",
    "上方程式確保模型可正常訓練及進行預測，\n",
    "且皆固定 Random Seed 為 8863，\n",
    "下方則可於上方程式執行後，確保已執行 get_model.sh 下載比賽最終使用的 6 個模型，\n",
    "直接執行取得後處理前的最終預測結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path='./Predictions_Ensemble6_th043/'\n",
    "os.makedirs(out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = torch.load('./Models/Deeplabv3P_resnet50_batch16.pth')\n",
    "resnet101 = torch.load('./Models/Deeplabv3P_resnet101_batch12.pth')\n",
    "resnet152 = torch.load('./Models/Deeplabv3P_resnet152_batch8.pth')\n",
    "resnext50 = torch.load('./Models/Deeplabv3P_resnext50_batch12.pth')\n",
    "resnext101 = torch.load('./Models/Deeplabv3P_resnext101_batch8.pth')\n",
    "se_resnet50 = torch.load('./Models/Deeplabv3P_se_resnet50_batch12.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(inference_dataset)):\n",
    "    name = os.path.basename(inference_dataset.images_fps[i])\n",
    "    image_vis = inference_dataset[i][0]\n",
    "    image = inference_dataset[i]\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        pr_mask1 = resnet50(x_tensor)\n",
    "        pr_mask2 = resnet101(x_tensor)\n",
    "        pr_mask3 = resnet152(x_tensor)\n",
    "        pr_mask4 = resnext50(x_tensor)\n",
    "        pr_mask5 = resnext101(x_tensor)\n",
    "        pr_mask6 = se_resnet50(x_tensor)\n",
    "    pr_mask = (pr_mask1 + pr_mask2 + pr_mask3 + pr_mask4 + pr_mask5 + pr_mask6) / 6\n",
    "    pr_mask += 0.07\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "    zeros = np.zeros((800, 1600, 3))\n",
    "    zeros[...,0] = pr_mask\n",
    "    zeros[...,1] = pr_mask\n",
    "    zeros[...,2] = pr_mask\n",
    "    zeros = cv2.resize(zeros, (1716, 942))\n",
    "    plt.imsave(os.path.join(out_path, name.replace('.jpg','.png')), zeros)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "MD984zz5e6jE"
   ],
   "name": "Unet.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "6a926afa313b26ae1264fdcf81c726a97e69f6ba2ba780f6aa901948710f8d6e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e831a91d8194e89bb740cbceb163cae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45a55adfb6de48a7a32ce973b0c3189f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0e303920ced45299fdc5d377102b98a",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51ad174d322c4b9e81b19f7c6a7aac5b",
      "value": 102502400
     }
    },
    "51ad174d322c4b9e81b19f7c6a7aac5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6ae0b4bf834e462bbe9fb0fa3cbd3165": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8cb854c2539c4aaab343b3750e20533b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9da2906e168f48d8bb8053ae8f177c8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a83345383dab49d1997ed5cd62cf3ea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f24b3fc929594204b7016a62ae601213",
      "placeholder": "​",
      "style": "IPY_MODEL_0e831a91d8194e89bb740cbceb163cae",
      "value": " 97.8M/97.8M [00:01&lt;00:00, 72.9MB/s]"
     }
    },
    "e76cab6c71514109b993416a53c2a624": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8cb854c2539c4aaab343b3750e20533b",
      "placeholder": "​",
      "style": "IPY_MODEL_6ae0b4bf834e462bbe9fb0fa3cbd3165",
      "value": "100%"
     }
    },
    "ec5f81c5a86448c1a7ff5913e9e26ef3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e76cab6c71514109b993416a53c2a624",
       "IPY_MODEL_45a55adfb6de48a7a32ce973b0c3189f",
       "IPY_MODEL_a83345383dab49d1997ed5cd62cf3ea7"
      ],
      "layout": "IPY_MODEL_9da2906e168f48d8bb8053ae8f177c8c"
     }
    },
    "f0e303920ced45299fdc5d377102b98a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f24b3fc929594204b7016a62ae601213": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
